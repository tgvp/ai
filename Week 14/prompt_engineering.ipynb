{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e256b4e",
      "metadata": {
        "id": "4e256b4e"
      },
      "source": [
        "# Engenharia de Prompt com LLMs\n",
        "\n",
        "Este notebook explora os principais conceitos e estratÃ©gias de **Prompt Engineering**, com teoria, exemplos prÃ¡ticos e exercÃ­cios interativos baseados no [Prompting Guide](https://www.promptingguide.ai)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "010a3a06",
      "metadata": {
        "id": "010a3a06"
      },
      "source": [
        "## O que Ã© Engenharia de Prompt?\n",
        "Engenharia de prompt Ã© o processo de projetar instruÃ§Ãµes eficazes para modelos de linguagem como o GPT para que eles realizem tarefas especÃ­ficas. Envolve a formulaÃ§Ã£o de entradas (prompts) que maximizam a qualidade das respostas geradas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef550821",
      "metadata": {
        "id": "ef550821"
      },
      "source": [
        "## AplicaÃ§Ãµes\n",
        "- Chatbots\n",
        "- GeraÃ§Ã£o de cÃ³digo\n",
        "- TraduÃ§Ã£o e sumarizaÃ§Ã£o\n",
        "- ClassificaÃ§Ã£o de sentimentos\n",
        "- ExtraÃ§Ã£o de informaÃ§Ãµes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b371da8e",
      "metadata": {
        "id": "b371da8e"
      },
      "source": [
        "## Elementos de um prompt\n",
        "\n",
        "Um prompt pode conter qualquer um dos seguintes componentes:\n",
        "\n",
        "InstruÃ§Ã£o - uma tarefa ou instruÃ§Ã£o especÃ­fica que vocÃª deseja que o modelo execute\n",
        "\n",
        "Contexto - pode envolver informaÃ§Ãµes externas ou contexto adicional que pode direcionar o modelo para melhores respostas\n",
        "\n",
        "Dados de entrada - Ã© a entrada ou pergunta para a qual estamos interessados em encontrar uma resposta\n",
        "\n",
        "Indicador de saÃ­da - indica o tipo ou formato da saÃ­da."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a734f22",
      "metadata": {
        "id": "5a734f22"
      },
      "source": [
        "## Tipos de Prompting\n",
        "\n",
        "### Zero-shot Prompting\n",
        "- nÃ£o fornecemos nenhum exemplo ao modelo\n",
        "\n",
        "Ãštil quando a tarefa Ã© clara por si sÃ³."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67aee04",
      "metadata": {
        "id": "a67aee04"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classifique o texto em neutro, negativo ou positivo.\n",
        "Texto: Acho que as fÃ©rias estÃ£o boas.\n",
        "Sentimento:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c005371",
      "metadata": {
        "id": "3c005371"
      },
      "source": [
        "**SaÃ­da do ChatGPT**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d44b06",
      "metadata": {
        "id": "29d44b06"
      },
      "source": [
        "Sentimento: positivo âœ…\n",
        "\n",
        "Justificativa: A expressÃ£o \"as fÃ©rias estÃ£o boas\" transmite uma avaliaÃ§Ã£o favorÃ¡vel, ainda que moderada, indicando uma experiÃªncia agradÃ¡vel. O uso de \"acho\" suaviza um pouco, mas nÃ£o altera o tom geral positivo da frase."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0093ad",
      "metadata": {
        "id": "ad0093ad"
      },
      "source": [
        "Quando o `zero shot` nÃ£o funciona, Ã© recomendÃ¡vel fornecer demonstraÃ§Ãµes ou exemplos no prompt que levam ao prompt de `few shot`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a84932",
      "metadata": {
        "id": "35a84932"
      },
      "source": [
        "### Few shot\n",
        "- fornecemos alguns exemplos de entrada e saÃ­da\n",
        "- uma tÃ©cnica para permitir o aprendizado no contexto, fornecemos demonstraÃ§Ãµes para direcionar o modelo e obter uma resposta melhor\n",
        "\n",
        "Ãštil quando a tarefa Ã© complexa ou o modelo precisa de contexto adicional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6313e05e",
      "metadata": {
        "id": "6313e05e"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Um \"whatpu\" Ã© um pequeno animal peludo nativo da TanzÃ¢nia. Exemplo de frase que usa\n",
        "a palavra whatpu Ã©:\n",
        "EstÃ¡vamos viajando pela Ãfrica e vimos esses whatpus muito fofos.\n",
        "\"Farduddlear\" significa pular para cima e para baixo muito rÃ¡pido. Exemplo de frase que usa\n",
        "a palavra farduddlear Ã©:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0e62e2",
      "metadata": {
        "id": "ec0e62e2"
      },
      "source": [
        "**SaÃ­da do ChatGPT**\n",
        "\n",
        "Exemplo de frase que usa a palavra farduddlear:\n",
        "\n",
        "As crianÃ§as comeÃ§aram a farduddlear de empolgaÃ§Ã£o quando viram os presentes de aniversÃ¡rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ee193b",
      "metadata": {
        "id": "b7ee193b"
      },
      "outputs": [],
      "source": [
        "prompt = \"Traduza: 'gato' -> 'cat', 'cachorro' -> 'dog', 'pÃ¡ssaro' ->\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82e5325",
      "metadata": {
        "id": "d82e5325"
      },
      "source": [
        "**SaÃ­da do ChatGPT**\n",
        "\n",
        "'pÃ¡ssaro' -> 'bird' ðŸ¦"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16ebb0f",
      "metadata": {
        "id": "b16ebb0f"
      },
      "source": [
        "### Chain of Thought\n",
        "- estimula o modelo a pensar passo a passo\n",
        "- permite recursos de raciocÃ­nio complexos por meio de etapas intermediÃ¡rias de raciocÃ­nio.\n",
        "\n",
        "Ãštil para tarefas que exigem raciocÃ­nio lÃ³gico ou mÃºltiplas etapas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec666289",
      "metadata": {
        "id": "ec666289"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Se JoÃ£o tem 3 maÃ§Ã£s e ganha mais 2, quantas ele tem?\n",
        "Vamos pensar passo a passo.\"\n",
        "JoÃ£o tem 3 maÃ§Ã£s. Ele ganha mais 2. 3 + 2 = 5. Resposta: 5\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3347c162",
      "metadata": {
        "id": "3347c162"
      },
      "source": [
        "**SaÃ­da do ChatGPT**\n",
        "\n",
        "Correto! âœ…\n",
        "\n",
        "Vamos revisar passo a passo:\n",
        "\n",
        "JoÃ£o tem 3 maÃ§Ã£s.\n",
        "\n",
        "Ele ganha mais 2 maÃ§Ã£s.\n",
        "\n",
        "Fazemos a conta: 3 + 2 = 5\n",
        "\n",
        "Resposta: 5 maÃ§Ã£s ðŸŽðŸŽðŸŽðŸŽðŸŽ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c9a708",
      "metadata": {
        "id": "54c9a708"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Os nÃºmeros Ã­mpares neste grupo somam um nÃºmero par: 4, 8, 9, 15, 12, 2, 1.\n",
        "R: Somando todos os nÃºmeros Ã­mpares (9, 15, 1) dÃ¡ 25. A resposta Ã© Falso.\n",
        "Os nÃºmeros Ã­mpares neste grupo somam um nÃºmero par: 17, 10, 19, 4, 8, 12, 24.\n",
        "R: Somando todos os nÃºmeros Ã­mpares (17, 19) dÃ¡ 36. A resposta Ã© Verdadeiro.\n",
        "Os nÃºmeros Ã­mpares neste grupo somam um nÃºmero par: 16, 11, 14, 4, 8, 13, 24.\n",
        "R: Somando todos os nÃºmeros Ã­mpares (11, 13) dÃ¡ 24. A resposta Ã© Verdadeiro.\n",
        "Os nÃºmeros Ã­mpares neste grupo somam um nÃºmero par: 17, 9, 10, 12, 13, 4, 2.\n",
        "R: Somando todos os nÃºmeros Ã­mpares (17, 9, 13) dÃ¡ 39. A resposta Ã© Falso.\n",
        "Os nÃºmeros Ã­mpares neste grupo somam um nÃºmero par: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5389ee16",
      "metadata": {
        "id": "5389ee16"
      },
      "source": [
        "**SaÃ­da do ChatGPT**\n",
        "\n",
        "Vamos somar os nÃºmeros Ã­mpares do grupo:\n",
        "15, 5, 13, 7, 1\n",
        "\n",
        "Agora, somando:\n",
        "\n",
        "15 + 5 = 20\n",
        "\n",
        "20 + 13 = 33\n",
        "\n",
        "33 + 7 = 40\n",
        "\n",
        "40 + 1 = 41\n",
        "\n",
        "41 Ã© um nÃºmero Ã­mpar, portanto:\n",
        "\n",
        "A resposta Ã© Falso. âŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0576514d",
      "metadata": {
        "id": "0576514d"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Fui ao mercado e comprei 10 maÃ§Ã£s. Dei 2 maÃ§Ã£s ao vizinho e 2 ao reparador. EntÃ£o fui comprar mais 5 maÃ§Ã£s e comi 1. Com quantas maÃ§Ã£s fiquei?\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "361aadd1",
      "metadata": {
        "id": "361aadd1"
      },
      "source": [
        "**SaÃ­da do ChatGPT**\n",
        "\n",
        "Vamos resolver passo a passo:\n",
        "\n",
        "VocÃª comprou 10 maÃ§Ã£s.\n",
        "\n",
        "Deu 2 ao vizinho â†’ 10 - 2 = 8\n",
        "\n",
        "Deu 2 ao reparador â†’ 8 - 2 = 6\n",
        "\n",
        "Comprou mais 5 maÃ§Ã£s â†’ 6 + 5 = 11\n",
        "\n",
        "Comeu 1 maÃ§Ã£ â†’ 11 - 1 = 10\n",
        "\n",
        "Resposta: VocÃª ficou com 10 maÃ§Ã£s. ðŸŽðŸŽðŸŽðŸŽðŸŽðŸŽðŸŽðŸŽðŸŽðŸŽ âœ…"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a2974a",
      "metadata": {
        "id": "f2a2974a"
      },
      "source": [
        "`Reasoning`: Ã© a capacidade de raciocinar e explicar o processo de pensamento por trÃ¡s da resposta.\n",
        "\n",
        "`Alumination`: quando o modelo responde de forma errada, desvirtuand o raciocÃ­nio. Normalmente requer correÃ§Ãµes e ajustes no prompt de forma a guiar o modelo para a resposta correta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a0f72b",
      "metadata": {
        "id": "53a0f72b"
      },
      "source": [
        "## Extra - Experimentos usando LLMs atravÃ©s de APIs e localmente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c2abfde",
      "metadata": {
        "id": "4c2abfde"
      },
      "source": [
        "**Vamos testar um pouco com o Gemini :)**\n",
        "\n",
        "[https://aistudio.google.com/](https://aistudio.google.com/)\n",
        "\n",
        "**ObservaÃ§Ã£o**: Nunca compartilhe suas chaves de API ou tokens de acesso publicamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cb9d5a",
      "metadata": {
        "id": "14cb9d5a"
      },
      "outputs": [],
      "source": [
        "!pip install -q python-dotenv google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9caaed2",
      "metadata": {
        "id": "e9caaed2"
      },
      "source": [
        "Importando variÃ¡veis de ambiente para acessar a chave da API do Gemini\n",
        "- coloque seu ficheiro .env na mesma pasta do notebook\n",
        "- lembre-se de manter sua API Key em segredo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4345c139",
      "metadata": {
        "id": "4345c139"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Carrega as variÃ¡veis do .env\n",
        "load_dotenv(\".env\")\n",
        "\n",
        "# Pega a chave da API\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c285ceb",
      "metadata": {
        "id": "8c285ceb"
      },
      "source": [
        "Listando os modelos disponÃ­veis no Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZR-KhdvL7JVe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "ZR-KhdvL7JVe",
        "outputId": "5c78f721-ab8c-4bb7-a915-f0fd81d0e474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/veo-2.0-generate-001\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# verificando os modelos disponÃ­veis\n",
        "models = genai.list_models()\n",
        "\n",
        "for model in models:\n",
        "    print(model.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef10dde",
      "metadata": {
        "id": "bef10dde"
      },
      "source": [
        "Testando o modelo com um prompt simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cWUDR5iu7RkW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "cWUDR5iu7RkW",
        "outputId": "fa3fc899-81db-458b-dda7-9b48acb5002f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engenharia de prompts Ã© a arte e a ciÃªncia de criar prompts (instruÃ§Ãµes) que geram as melhores respostas possÃ­veis de um modelo de linguagem grande (LLM), como o ChatGPT, Bard, ou outros.  Ã‰ como \"programar\" o modelo usando linguagem natural, ao invÃ©s de cÃ³digo de programaÃ§Ã£o tradicional.  Em vez de dizer simplesmente \"escreva uma histÃ³ria\", vocÃª precisa \"engenhar\" seu prompt para obter o tipo de histÃ³ria que vocÃª deseja.\n",
            "\n",
            "**Exemplo para iniciantes:**\n",
            "\n",
            "Imagine que vocÃª quer uma histÃ³ria de ficÃ§Ã£o cientÃ­fica sobre um gato que viaja pelo espaÃ§o.  Um prompt ruim seria:\n",
            "\n",
            "**Prompt Ruim:** \"Escreva uma histÃ³ria.\"\n",
            "\n",
            "Esse prompt Ã© muito vago. O modelo pode escrever sobre qualquer coisa.\n",
            "\n",
            "Um prompt melhor, usando engenharia de prompts, seria:\n",
            "\n",
            "**Prompt Bom:** \"Escreva uma histÃ³ria de ficÃ§Ã£o cientÃ­fica sobre um gato siamÃªs chamado Cosmo que viaja pelo espaÃ§o em uma nave espacial feita de lata de sardinha. A histÃ³ria deve incluir um encontro com alienÃ­genas amigÃ¡veis e uma busca por um queijo cÃ³smico lendÃ¡rio.\"\n",
            "\n",
            "\n",
            "Veja a diferenÃ§a? O prompt \"bom\" fornece:\n",
            "\n",
            "* **Contexto:** FicÃ§Ã£o cientÃ­fica\n",
            "* **Personagem principal:** Um gato siamÃªs chamado Cosmo\n",
            "* **Detalhes:** Nave espacial feita de lata de sardinha\n",
            "* **Elementos da trama:** Encontro com alienÃ­genas amigÃ¡veis, busca por queijo cÃ³smico.\n",
            "\n",
            "Esse nÃ­vel de detalhe direciona o LLM para gerar uma histÃ³ria muito mais especÃ­fica e prÃ³xima do que vocÃª deseja.  Ã‰ a engenharia de prompts em aÃ§Ã£o!\n",
            "\n",
            "**Outros exemplos de tÃ©cnicas de engenharia de prompts:**\n",
            "\n",
            "* **Especificar o estilo:** \"Escreva um poema no estilo de Shakespeare sobre...\"\n",
            "* **Definir o tom:** \"Escreva um artigo de notÃ­cia com tom sÃ©rio sobre...\"\n",
            "* **Fornecer exemplos:** \"Escreva um resumo do livro '1984' no mesmo estilo do resumo deste livro [inserir resumo exemplo].\"\n",
            "* **Utilizar poucos e bons detalhes:** Em vez de dar muitos detalhes irrelevantes, foque nos elementos essenciais para a resposta desejada.\n",
            "* **IteraÃ§Ã£o:** Criar um prompt, testar a saÃ­da e refinar o prompt com base nos resultados.\n",
            "\n",
            "Quanto mais vocÃª praticar, melhor ficarÃ¡ em construir prompts que geram resultados incrÃ­veis.  A chave Ã© ser especÃ­fico e detalhista, guiando o LLM para a resposta desejada.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configurar Gemini\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Usar o modelo disponÃ­vel e funcional\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "# Definir um prompt\n",
        "prompt = \"Explique o que Ã© engenharia de prompts com um exemplo para iniciantes.\"\n",
        "\n",
        "# Gerar a resposta\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Exibir a resposta\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff356d10",
      "metadata": {
        "id": "ff356d10"
      },
      "source": [
        "Vamos testar agora localmente com o `LLama`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce98fc6",
      "metadata": {
        "id": "fce98fc6"
      },
      "source": [
        "LLM local com Mistral 7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "521cdaf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "521cdaf8",
        "outputId": "c574d7cb-bc02-4154-814e-445cf6ae6439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e7453a4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7453a4b",
        "outputId": "f31195f1-0155-4200-e0fe-e0a1157f426c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-26 00:32:24--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.38, 3.168.73.106, 3.168.73.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1748223144&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0ODIyMzE0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=E2zmSxWb8LtNuhTCk8bnDAlPOm0nsFOch57UXkTEYgOemao-SXLpc3nalbGxAj-rBjZPZ2rHi3kMg4QwtNHaaYknA058AHc0G8LhbPn2Mi8n%7EqdmVhuMSLr-m29G-CuvPAQdWTfx%7E-cwylYf7OEITBJ-fvQ2iuqxOrMztMiFVGhAWlGqBFVUrpAIxS2pQwo6IK9stuuasW%7Ex6Fp5-Lh1gYrUub5rhgG6p6fxaG-FwUVVdYS9nxL9pKDczPP47e8qH7WjGMvbOXqB2j3rrTRLt3vjyay3sxYmY5skUpzBndP0Za7eLgShbGdEf74YbxqJuQ2Czglz99cy1nJHr4VgXg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-05-26 00:32:25--  https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1748223144&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0ODIyMzE0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=E2zmSxWb8LtNuhTCk8bnDAlPOm0nsFOch57UXkTEYgOemao-SXLpc3nalbGxAj-rBjZPZ2rHi3kMg4QwtNHaaYknA058AHc0G8LhbPn2Mi8n%7EqdmVhuMSLr-m29G-CuvPAQdWTfx%7E-cwylYf7OEITBJ-fvQ2iuqxOrMztMiFVGhAWlGqBFVUrpAIxS2pQwo6IK9stuuasW%7Ex6Fp5-Lh1gYrUub5rhgG6p6fxaG-FwUVVdYS9nxL9pKDczPP47e8qH7WjGMvbOXqB2j3rrTRLt3vjyay3sxYmY5skUpzBndP0Za7eLgShbGdEf74YbxqJuQ2Czglz99cy1nJHr4VgXg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.171.139.125, 3.171.139.14, 3.171.139.118, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.171.139.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368438944 (4.1G) [binary/octet-stream]\n",
            "Saving to: â€˜models/mistral-7b-instruct.ggufâ€™\n",
            "\n",
            "models/mistral-7b-i 100%[===================>]   4.07G  88.5MB/s    in 30s     \n",
            "\n",
            "2025-05-26 00:32:55 (138 MB/s) - â€˜models/mistral-7b-instruct.ggufâ€™ saved [4368438944/4368438944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Criar diretÃ³rio\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Baixar modelo GGUF quantizado\n",
        "!wget -O models/mistral-7b-instruct.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5513d32a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5513d32a",
        "outputId": "b8b44c62-a6b9-4c66-81f1-acf35b1ac717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-instruct.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1637 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.24 B\n",
            "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "...................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 1024\n",
            "llama_context: n_ctx_per_seq = 1024\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 1024 (padded)\n",
            "llama_kv_cache_unified: kv_size = 1024, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
            "llama_kv_cache_unified: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context:        CPU compute buffer size =    98.01 MiB\n",
            "llama_context: graph nodes  = 1094\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "llama_perf_context_print:        load time =    6191.70 ms\n",
            "llama_perf_context_print: prompt eval time =    6191.48 ms /    19 tokens (  325.87 ms per token,     3.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =   57020.44 ms /    99 runs   (  575.96 ms per token,     1.74 tokens per second)\n",
            "llama_perf_context_print:       total time =   63269.90 ms /   118 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Resposta: Engenharia de prompts Ã© um processo de pensar e planejar o que queremos dizer em linguagem simples. Ã‰ como se fosse um mapa para guiar a pessoa para que compreenda o que queremos dizer. O engenheiro de prompts tem que ser criativo e pensar em uma forma que serÃ¡ facilmente entendida por quem estÃ¡ ouvindo ou lendo. O\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"models/mistral-7b-instruct.gguf\",\n",
        "    n_ctx=1024,\n",
        "    n_threads=8,  # Ajuste conforme necessÃ¡rio\n",
        ")\n",
        "\n",
        "prompt = \"Explique o que Ã© engenharia de prompts em linguagem simples.\"\n",
        "\n",
        "output = llm(prompt, max_tokens=100)\n",
        "print(output[\"choices\"][0][\"text\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}