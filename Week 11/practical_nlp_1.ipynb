{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d147875",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (NLP)\n",
    "## Pré-processamento, Representações Vetoriais e Similaridade de Textos\n",
    "Neste notebook, você aprenderá a aplicar diversas técnicas de pré-processamento de texto, desde a implementação manual até o uso da biblioteca SpaCy. Também exploraremos diferentes formas de representar textos numericamente (BoW, TF-IDF) e como medir a similaridade entre documentos usando cosseno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e5f35",
   "metadata": {},
   "source": [
    "## 1. Lowercasing\n",
    "Converter o texto para letras minúsculas é o primeiro passo para padronizar os dados, evitando que palavras como \"Casa\" e \"casa\" sejam tratadas como diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0fb0f",
   "metadata": {},
   "source": [
    "### Exercício 1 - Lowercasing Manual\n",
    "Converta o texto abaixo para letras minúsculas usando Python puro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f72e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"O Gato Subiu NO Telhado.\"\n",
    "# TODO: converta para minúsculas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654369eb",
   "metadata": {},
   "source": [
    "### Exercício 2 - Lowercasing com SpaCy\n",
    "Repita a operação usando SpaCy e verifique os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a43f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(text)\n",
    "[token.text.lower() for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f5277",
   "metadata": {},
   "source": [
    "## 2. Remoção de Pontuação\n",
    "A pontuação normalmente não carrega significado semântico relevante e pode ser removida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8c36e",
   "metadata": {},
   "source": [
    "### Exercício 3 - Remoção Manual de Pontuação\n",
    "Remova manualmente a pontuação usando expressões regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259814b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Olá, tudo bem? Espero que sim! Vamos começar.\"\n",
    "# TODO: remova pontuações\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fe250",
   "metadata": {},
   "source": [
    "### Exercício 4 - Remoção de Pontuação com SpaCy\n",
    "Use SpaCy para ignorar tokens que são pontuação.\n",
    "\n",
    "Explore o `token.is_punct` para verificar se um token é pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729946b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Olá', 'tudo', 'bem', 'Espero', 'que', 'sim', 'Vamos', 'começar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aebb094",
   "metadata": {},
   "source": [
    "## 3. Tokenização\n",
    "Tokenizar é dividir o texto em unidades menores, geralmente palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce759ee",
   "metadata": {},
   "source": [
    "### Exercício 5 - Tokenização Manual\n",
    "Divida o texto em palavras usando `.split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dd2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Aprendizado de máquina é incrível\"\n",
    "# TODO: tokenize manualmente criando uma lista de tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f9a16",
   "metadata": {},
   "source": [
    "### Exercício 6 - Tokenização com SpaCy\n",
    "Tokenize o mesmo texto com SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fae26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aprendizado', 'de', 'máquina', 'é', 'incrível']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6c30b",
   "metadata": {},
   "source": [
    "## 4. Remoção de Stopwords\n",
    "Stopwords são palavras muito comuns que normalmente não carregam significado relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944e856",
   "metadata": {},
   "source": [
    "### Remoção Manual de Stopwords\n",
    "Use uma lista manual de stopwords para removê-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c267758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remova stopwords criando uma lista de palavras comuns que devem ser removidas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba520f4b",
   "metadata": {},
   "source": [
    "### Exercício 8 - Remoção de Stopwords com SpaCy\n",
    "Use SpaCy para remover automaticamente as stopwords.\n",
    "Explore o uso do `token.is_stop` para verificar se um token é uma stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f6f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gosto', 'estudar', 'dados']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Eu gosto de estudar dados\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f50fb5d",
   "metadata": {},
   "source": [
    "## 5. Lematização e Stemming\n",
    "\n",
    "- A lematização reduz uma palavra à sua forma canônica. Noutras palavras, transforma a palavra em sua raiz ou forma base.\n",
    "\n",
    "    - Exemplo 1: \"correndo\" se torna \"correr\".\n",
    "    - Exemplo 2: \"melhores\" se torna \"bom\".\n",
    "    - Exemplo 3: \"maçãs\" se torna \"maçã\".\n",
    "\n",
    "- Stemming é uma técnica semelhante, mas menos precisa, que corta os sufixos das palavras. Por exemplo, \"correndo\" se torna \"corr\".\n",
    "\n",
    "- Qual dos dois utilizar? A lematização é mais precisa, mas o stemming pode ser mais rápido. A escolha depende do seu caso de uso.\n",
    "\n",
    "**Imprtante: o Spacy não faz Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c35e8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menino', 'estar', 'correr', 'feliz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Os meninos estavam correndo felizes.\"\n",
    "doc = nlp(text)\n",
    "[token.lemma_ for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b10a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "# baixando o stemmer\n",
    "nltk.download('rslp')\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6f7dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menin', 'est', 'corr', 'feliz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems = [stemmer.stem(token.text) for token in doc if not token.is_stop and not token.is_punct]\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0444c",
   "metadata": {},
   "source": [
    "## 6. Representações Vetoriais\n",
    "Agora que temos textos pré-processados, precisamos representá-los numericamente para aplicar modelos.\n",
    "\n",
    "### 6.1 Corpus:\n",
    "Um corpus é uma coleção de documentos, usada como base para análise linguística e processamento de linguagem natural.\n",
    "\n",
    "Os próximos passos envolvem a construção de um corpus.\n",
    "\n",
    "Iremos criar um corpus, entender como o Bag of Words (BoW) funciona e como a representação TF-IDF pode ser aplicada.\n",
    "\n",
    "### 6.2 Bag of Words (BoW)\n",
    "Representa um texto como uma coleção de palavras, ignorando gramática e ordem, e focando apenas na frequência com que as palavras aparecem.\n",
    "\n",
    "1. **Contagem de Palavras**: Crie um dicionário que conta a frequência de cada palavra em um texto.\n",
    "2. **Representação BoW**: Construa uma matriz onde cada linha representa um documento e cada coluna representa uma palavra do vocabulário, preenchida com a contagem de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b48613",
   "metadata": {},
   "source": [
    "### Exercício 10 - Crie um Corpus com os seguintes documentos:\n",
    "\n",
    "- `\"gato gosta de peixe\",`\n",
    "- `\"cachorro gosta de osso\"`\n",
    "- `\"peixe e osso são alimentos\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f185cbd",
   "metadata": {},
   "source": [
    "### Exercício 11 - BoW from scratch\n",
    "\n",
    "Construa uma representação BoW para o corpus criado no exercício 10. Utilize um dicionário para contar a frequência de cada palavra e crie uma matriz onde cada linha representa um documento e cada coluna representa uma palavra do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ac501b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "\n",
    "# crie um vocabulario\n",
    "\n",
    "# crie a matriz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3af01",
   "metadata": {},
   "source": [
    "### Exercício 11 - BoW com CountVectorizer\n",
    "Use `CountVectorizer` para gerar a matriz BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "  \n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ea7df",
   "metadata": {},
   "source": [
    "### 6.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "Ajusta a frequência com base em quão rara uma palavra é em outros documentos.\n",
    "\n",
    "- **TF**: Frequência de uma palavra em um documento.\n",
    "- **IDF**: Inverso da frequência de documentos que contêm a palavra.\n",
    "- **TF-IDF**: Produto de TF e IDF, representando a importância de uma palavra em um documento em relação ao corpus.\n",
    "\n",
    "Pseudo-código para calcular TF-IDF:\n",
    "\n",
    "```\n",
    "para um documento d em documentos:\n",
    "    para cada palavra p em d:\n",
    "        tf = contagem de p em d / total de palavras em d\n",
    "        idf = log(total de documentos / contagem de documentos que contêm p)\n",
    "        tfidf[p] = tf * idf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53eac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b5d4002",
   "metadata": {},
   "source": [
    "### Exercício 12 - TF-IDF com TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459352ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cachorro' 'de' 'está' 'gato' 'gosta' 'jardim' 'no' 'peixe' 'telhado']\n",
      "[[0.         0.         0.46580855 0.46580855 0.         0.\n",
      "  0.46580855 0.         0.59081908]\n",
      " [0.55528266 0.         0.43779123 0.         0.         0.55528266\n",
      "  0.43779123 0.         0.        ]\n",
      " [0.         0.52547275 0.         0.41428875 0.52547275 0.\n",
      "  0.         0.52547275 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "print(tfidf.get_feature_names_out())\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc5093",
   "metadata": {},
   "source": [
    "## 7. Similaridade\n",
    "A similaridade cosseno mede o quão parecidos dois vetores são com base no ângulo entre eles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37437692",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{similaridade\\_cosseno}(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\cdot \\|\\vec{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "**Onde:**\n",
    "\n",
    "- O produto escalar dos vetores A e B mede o quanto os vetores \"apontam na mesma direção\".\n",
    "\n",
    "- ∥A∥ e ∥B∥: são os módulos (ou magnitudes) dos vetores A e B. Isso normaliza os vetores, ou seja, ignora o tamanho e foca na direção\n",
    "\n",
    "Imagine dois vetores apontando em direções diferentes.\n",
    "\n",
    "- Quanto mais próximos eles estiverem, **menor o ângulo entre eles**, o que resulta em uma **maior similaridade cosseno**.\n",
    "\n",
    "- Se eles apontam para a **mesma direção**, a similaridade cosseno será **1 (máxima similaridade)**.\n",
    "\n",
    "- Se forem **ortogonais (formam 90 graus)**, a similaridade será **0 (sem similaridade)**.\n",
    "\n",
    "- Se forem **opostos**, o valor será **-1 (totalmente opostos)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9a6ef",
   "metadata": {},
   "source": [
    "### Exercício 12 - Crie uma função para calcular a similaridade cosseno entre dois vetores.\n",
    "\n",
    "dica: use a biblioteca `numpy` para facilitar os cálculos.\n",
    "\n",
    "`np.dot` e `np.linalg.norm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vetor_A = np.array([1, 2, 3])\n",
    "vetor_B = np.array([4, 5, 6])\n",
    "\n",
    "# calcule o produto escalar\n",
    "\n",
    "# calcule a norma do vetor_A\n",
    "\n",
    "# calcule a norma do vetor_B\n",
    "\n",
    "# similaridade_cosseno = produto_escalar / norma_A * norma_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe28a08",
   "metadata": {},
   "source": [
    "### Exercício 14 - Teste sua função agora com os vetores abaixo:\n",
    "\n",
    "```python\n",
    "vetor_A = np.array([1, 2, 3])\n",
    "vetor_B = np.array([1, 2, 6])\n",
    "```\n",
    "\n",
    "**Notou alguma diferença em relação ao exercício anterior?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_A = np.array([1, 2, 3])\n",
    "vetor_B = np.array([1, 2, 6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78046dcf",
   "metadata": {},
   "source": [
    "### Exercício 15 - Agora aplique a função de similaridade cosseno entre os vetores gerados pelo `CountVectorizer` e `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fbfd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
